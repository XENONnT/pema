{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peak matching for optimizing clustering and classification of XENONnT\n",
    "\n",
    "Angevaare, Joran <j.angevaare@nikhef.nl> \n",
    "\n",
    "2021-04-22\n",
    "\n",
    "## This notebook\n",
    "In order to update clustering and classification parameters, this notebook investigates two properties of a simulated set of data:\n",
    " - `bias`, the area of the reconstructed peak / the area of the simulated peak\n",
    " - `arbitrary_acceptance`, slightly modified acceptance definition in order to allow penalty factors for bad reconstruction outcomes. The acceptance of the peaks is defined and explained in the [documentation](https://pema.readthedocs.io/en/latest/)\n",
    " \n",
    "Using a grid scan of the relevant clustering and classification parameters, an optimum configuration can be extracted. This improved, changed configuration is finally compared to the current \"default\" reconstruction/\n",
    "\n",
    "## Prerequisites \n",
    "#### Software\n",
    "The following packages are needed\n",
    " - strax\n",
    " - straxen\n",
    " - wfsim\n",
    " - pema\n",
    "\n",
    "#### Environment\n",
    "This notebook can either be run on a local machine but is best run on the dali server\n",
    "\n",
    "#### User input\n",
    "The user needs to change few things, as the notebook is designed to work out of the box. Only few places (explicitly marked <span style=\"color:red\">**user input required**</span> need changing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start up ``strax(en) + wfsim + pema`` + load tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pema\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('init.py'):\n",
    "    init = os.path.join(pema.__path__[0], '..', 'bin', \"pema_init.py\")\n",
    "    !ln -s $init init.py\n",
    "%run init.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">**User input required**</span> \n",
    "## Initialize the wavefrom simulator with your instructions\n",
    "\n",
    "Initialize either of\n",
    " - [Low E](setup_lowe.py), energies between 0-50/250 PE (S1/S2). Follow this link to [edit](/edit/pema/notebooks/setup_lowe.py).\n",
    " - [High E](setup_highe.py), energies between 0-1e5 PE (S1 and S2). Follow this link to [edit](/edit/pema/notebooks/setup_highe.py).\n",
    " - [Kr](setup_kr.py), Kr-double scatters only (S1 and S2). Follow this link to [edit](/edit/pema/notebooks/setup_kr.py).\n",
    " \n",
    "Each file contains the paths to a few folders and the setting up of a notebook on the dali computing cluster. Please change the following:\n",
    " - Folders (`base_dir, data_name, fig_dir, data_dir, raw_data_dir`)\n",
    " - Notebook starting requirements (`environ_init`)\n",
    " - The run_list of runs to simulated needed for gains etc. (`run_list`)\n",
    " - The instructions of the simulation itself such as #events, energies etc. (`instructions`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup_lowe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_config = {}\n",
    "\n",
    "def get_context(skip=None):\n",
    "    \"\"\"\n",
    "    Get a context and set all of the entries in the testing_config to the \n",
    "    context config IF they are not in 'skip' \n",
    "    \"\"\"\n",
    "    # Directories are set in the \"%run setup_...\" above\n",
    "    st = pema.pema_context(base_dir=base_dir,\n",
    "                           config_update=config_update,\n",
    "                           raw_dir=raw_data_dir,\n",
    "                           data_dir=data_dir)\n",
    "\n",
    "    # Update the context config with all of the parameters that are not skipped\n",
    "    for k, v in testing_config.items():\n",
    "        if skip is not None and k in skip:\n",
    "            print(f'skip {k}')\n",
    "        else:\n",
    "            st.set_config({k:v})\n",
    "\n",
    "    # Don't allow lazy mode for any config\n",
    "    st.set_context_config(\n",
    "        {'allow_shm': True, 'allow_lazy': False,\n",
    "         'timeout': 300, 'max_messages': 10,\n",
    "        })\n",
    "    return st\n",
    "\n",
    "st = get_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate raw records\n",
    "Below, we are going to simulate a number of runs, each run is processed in a different job on dali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of the running jobs in a list\n",
    "job_registry=[]\n",
    "\n",
    "for r in run_list:\n",
    "    print(f'Starting a job to simulate data for run {r}')\n",
    "    job = pema.ProcessRun(st, \n",
    "                          run_id=r, \n",
    "                          target=('records', 'peaklets'), \n",
    "                          config={})\n",
    "    job_registry.append(job)\n",
    "    cmd, job_name = job.make_cmd()\n",
    "    \n",
    "    # Alternatively look at job.exec_local\n",
    "    job.exec_dali(cmd, job_name, environ_init, mem=30_000, max_hours='08:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check how the jobs are doing by checking their logs\n",
    "for j in job_registry:\n",
    "    log=j.log_file\n",
    "    !tail -10 $log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we simply have to wait for the runs to finish. We can see if all the data is stored by generating a table of stored items as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_writer = pema.ProcessRun(\n",
    "    st,\n",
    "    run_list,\n",
    "    ('raw_records', 'records', 'peaklets', 'peaks_matched'))\n",
    "script_writer.all_stored()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to go\n",
    "Great, we are ready to go, we can do two things:\n",
    " 1. We can first try to find an optimal parameter set using the sections below\n",
    " 2. If we already know what parameter set we are going to need, we might just as well continue straight to:\n",
    " \n",
    " [**The validation section at the bottom of the notebook**](#Select-custom-config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make configs for the parameters to change\n",
    "Let's loop over the options we want to scan. We will write those to a dict and work from there.\n",
    "\n",
    "<span style=\"color:red\">**User input may be required**</span> depending on if one wants to look at some other parameters. Below, we will simply look at close to **all** of the clustering and classification parameters but this many naturally be overkill. If one simply wants to look at a single parameter, this can be greatly simplified.\n",
    "\n",
    "Let's load all the parameters related to peaks. Those are the ones we can optimize. We want to know their current value so that we can look around to see how things change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = st.show_config('peaklets')\n",
    "# Optionally, one may look parameters applying to peaklets by commenting out line below\n",
    "# opts = opts[opts['applies_to'] == ('peaklets', 'lone_hits')]\n",
    "opts = opts.to_records()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the current config, if there is an `current` option overwriting the `default`, do so here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = opts['option']\n",
    "values = opts['default']\n",
    "current = opts['current']\n",
    "mask = current != '<OMITTED>'\n",
    "values[mask] = current[mask]\n",
    "res = {k: v for (k, v) in zip(options, values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's hard code for a second which parameters we are interested in. Most of the other parameters are booleans etc. which are not suitable for this parameter scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_keys = [\n",
    " 'peaklet_gap_threshold',\n",
    " 'peak_left_extension',\n",
    " 'peak_right_extension',\n",
    " 'peak_split_filter_wing_width',\n",
    " 'peak_split_min_area',\n",
    " 'peak_split_iterations',\n",
    " 'tight_coincidence_window_left',\n",
    " 'tight_coincidence_window_right',\n",
    " 's1_max_rise_time',\n",
    " 's1_max_rise_time_post100',\n",
    " 's2_merge_max_area',\n",
    " 's2_merge_max_gap',\n",
    " 's2_merge_max_duration'\n",
    "]\n",
    "res = {k: v for (k,v )in res.items() if k in keep_keys}\n",
    "summary_config = res.copy()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_conf(conf):\n",
    "    \"\"\"\n",
    "    Check right and left extension to be allowable. That is, we obey:\n",
    "    gap_threshold > left_extension + right_extension\n",
    "    \"\"\"\n",
    "    gap_threshold = conf.get('peaklet_gap_threshold', \n",
    "                             res.get('peaklet_gap_threshold', 700))\n",
    "    left_extension = conf.get('peak_left_extension', \n",
    "                              res.get('peak_left_extension', 30))\n",
    "    right_extension = conf.get('peak_right_extension', \n",
    "                               res.get('peak_right_extension', 350))\n",
    "    return gap_threshold > left_extension + right_extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's make a list of configs we want to validate\n",
    "To this end, we loop over all the configs selected above and vary the value of each parameter in a simple for loop. The goal is to vary around the current value to allow a large parameter scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_tot = [{}]\n",
    "i=0\n",
    "keys_seen = []\n",
    "_st = st.new_context()\n",
    "\n",
    "# Loop over teh configs and add new configs to the config list\n",
    "for k, v in summary_config.items():\n",
    "    if k == 'peak_split_gof_threshold': \n",
    "        continue\n",
    "    _type = type(v)\n",
    "    \n",
    "    for factor in [2.5, 2, 1.5, 1.25, 1.1, 1,\n",
    "                   1/1.1, 1/1.25, 1/1.5, 1/2, 1/2.5    ]:\n",
    "        value = _type(v*factor)\n",
    "        conf = {k: value}\n",
    "        if not validate_conf(conf):\n",
    "            print(f'skip {conf}')\n",
    "            continue\n",
    "        # Let's check that this config is not already seen, otherwise just\n",
    "        # continue\n",
    "        _st.set_config(conf)\n",
    "        ev_key = _st.key_for('0', 'events')\n",
    "\n",
    "        if str(ev_key) in keys_seen and factor != 1:\n",
    "            continue\n",
    "        else:\n",
    "            # New config! Let's store it\n",
    "            conf_tot.append(conf)\n",
    "            keys_seen.append(str(ev_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brute force method for varying the `peak_split_gof_threshold`. This parameter (used in natural break splitting) is a `2 *4` tuple and therefor, many things can be optimized. \n",
    "\n",
    "The loop below is extremely brute force but does the trick. Keep in mind that looping over 8 parameters very very quickly becomes a huge set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factors = [1.5, \n",
    "#            1.25, 1, 0.8, \n",
    "#            0.5\n",
    "# ]\n",
    "# few_factors = [1.25, 1, 0.8]\n",
    "# for fa in factors:\n",
    "#     for fb in factors:\n",
    "#         for fc in few_factors:\n",
    "#             for fd in few_factors:\n",
    "#                 for fx in few_factors:\n",
    "#                     for fy in factors:\n",
    "#                         for fz in few_factors:\n",
    "#                             for fw in few_factors:\n",
    "#                                 if not fa+fb+fc+fd+fx+fy+fz+fw in 7 + np.array(factors):\n",
    "#                                     continue\n",
    "#                                 if fa >1 or fb>1 or fc>1 or fd>1:\n",
    "#                                     continue\n",
    "# #                                 if fc!=1 or fw != 1:\n",
    "# #                                     continue\n",
    "#                                 value = (None, \n",
    "#                                          ((0.5*fx, 1*fa), (6.0*fy, 0.4*fb)), \n",
    "#                                          ((2.*fz, 1*fc), (4.5*fw, 0.4*fd)))\n",
    "#                                 conf = {'peak_split_gof_threshold': value}\n",
    "#                                 _st.set_config(conf)\n",
    "#                                 ev_key = _st.key_for('0', 'events')\n",
    "\n",
    "#                                 if str(ev_key) in keys_seen:\n",
    "#                                     continue\n",
    "#                                 else:\n",
    "#                                     conf_tot.append(conf)\n",
    "#                                     keys_seen.append(str(ev_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs = tuple(conf_tot.copy())\n",
    "print(f'Working with {len(conf_tot)} configs, see below\\n{confs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit jobs for computing the results of configs\n",
    "Now, the fun bit starts, let's run each of the configs in a seperate job. \n",
    "\n",
    "Make sure that the raw-records simulations have finished first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# only use runs where we have the records stored\n",
    "selected_runs = [r for r in run_list if st.is_stored(r, 'records')]\n",
    "print(f'Doing runs:\\n{selected_runs}\\n{len(selected_runs)/len(run_list)*100:.1f}%')\n",
    "all_runs = len(selected_runs) == len(run_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the configs (this allows us to stop the submit-cell and restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_confs = iter(confs)\n",
    "job_registry = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters needed for the job submission\n",
    "target = ('raw_records', 'records',  \n",
    "          'peaklets', 'peak_basics', 'match_acceptance_extended',)\n",
    "RAM = 6_000\n",
    "queue_max = 110\n",
    "check_que_after = 10\n",
    "part = 'xenon1t'\n",
    "max_hours='01:00:00'\n",
    "\n",
    "# start the jobs one by one\n",
    "for i, conf in enumerate(tqdm(\n",
    "    iter_confs, \n",
    "    total = len(confs) - len(job_registry ), \n",
    "    desc='configs')):\n",
    "    \n",
    "    job = pema.ProcessRun(st, run_id=selected_runs, target=target, config=conf)\n",
    "    cmd, job_name = job.make_cmd()\n",
    "\n",
    "    job_registry.append(job)\n",
    "    # Submit the job if some of the targets is not stored yet.\n",
    "    if not job.all_stored(return_bool=True):\n",
    "        # one could clean up some data if some run keeps failing to process correctly\n",
    "        # job.purge_below('peak_basics')\n",
    "        # job.purge_below('match_acceptance_extended')\n",
    "        job.exec_dali(cmd, job_name, environ_init, mem=RAM, \n",
    "                      max_hours=max_hours, partition=part)\n",
    "\n",
    "    # Check if we are not getting too many jobs\n",
    "    if i % check_que_after:\n",
    "        q = !squeue -u `echo $USER`\n",
    "        while len(q)> queue_max:\n",
    "            q = !squeue -u `echo $USER`\n",
    "            print(f'waiting 10s, queue is full. {len(q)}')\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the runs to finish before continueing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    nrun = !squeue -u $USER  | wc -l\n",
    "    nrun = int(nrun[0])\n",
    "    print(nrun)\n",
    "    # if we have less than 5 jobs running, we are probably done, otherwise, wait\n",
    "    if nrun < 5:\n",
    "        break\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if everything is stored (can take a *very* long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 500)\n",
    "pd.concat([j.all_stored(show_key=True) for j in job_registry])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the outcomes of the simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acceptance(data):\n",
    "    \"\"\"\n",
    "    Simple function to calculate the mean acceptance fraction of an entire \n",
    "    dataset (incl. the binom. error)\n",
    "    \"\"\"\n",
    "    total = len(data)\n",
    "    found = np.sum(data['acceptance_fraction'])\n",
    "    return found/total, pema.binom_interval(found, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bias(data):\n",
    "    \"\"\"\n",
    "    Simple function to calculate the mean bias of an entire dataset (incl. std)\n",
    "    \"\"\"\n",
    "    total = len(data)\n",
    "    sub_sel = data['rec_bias'] > 0 \n",
    "    return np.mean(data['rec_bias'][sub_sel]), np.std(data['rec_bias'][sub_sel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the results\n",
    "\n",
    "We loop over each of the jobs, if they are finished, let's check the mean acceptance and mean bias of that configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = defaultdict(list)\n",
    "\n",
    "for i, job in enumerate(tqdm(job_registry)):\n",
    "    config = job.config\n",
    "    if config in res['config']:\n",
    "        continue\n",
    "    elif not job.all_stored(return_bool=True):\n",
    "        print(f'skip {job}')\n",
    "        continue\n",
    "    \n",
    "    data = job.st.get_array(selected_runs, \n",
    "                            'match_acceptance_extended', \n",
    "                            progress_bar=False)\n",
    "\n",
    "    # Parse the average mean and acceptance and store in our \"res\" list\n",
    "    res['number'].append(i)\n",
    "    res['config'].append(config)\n",
    "    res['config_type'].append(list(config.keys()))\n",
    "    for si in range(1,3):\n",
    "        sel = data['type'] == si\n",
    "        acceptance, (low, high) = compute_acceptance(data[sel])\n",
    "        res[f's{si}_acc'].append(acceptance)\n",
    "        res[f's{si}_acc_err'].append([acceptance-low, high-acceptance])\n",
    "\n",
    "        bias_mean, bias_err = compute_bias(data[sel])\n",
    "        res[f's{si}_bias'].append(bias_mean)\n",
    "        res[f's{si}_bias_err'].append(bias_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret parameter scan\n",
    "For each of the configs, we are going to grep the results from our dataframe above to see how the accepance/bias was affected by this parameter change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Common parameters for plot: S1/S2 colors and errorbar options\n",
    "colors = ('#1f77b4', '#ff7f0e')\n",
    "plot_kwargs = dict(markersize= 5,  ls='none', capsize=3, marker='o')\n",
    "\n",
    "for config_type in np.unique(df['config_type'].values):\n",
    "    if not config_type:\n",
    "        continue\n",
    "    print(config_type)\n",
    "    \n",
    "    # Select the configurations from the results dataframe\n",
    "    mask = np.array([c == config_type for c in  df['config_type']])\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(1*np.sum(mask),6))\n",
    "    plt.title(f'S1/S2 acceptance - {config_type}')\n",
    "    for axi, si in enumerate([1, 2]):\n",
    "        if axi ==1:\n",
    "            plt.sca(plt.gca().twinx())\n",
    "            plt.xticks()\n",
    "        plt.errorbar(df[mask]['number'], \n",
    "                     df[mask][f's{si}_acc'], \n",
    "                     yerr=np.array([e for e in df[mask][f's{si}_acc_err']]).T,\n",
    "                     label = f'S{si} acceptance',\n",
    "                     c = colors[axi],\n",
    "                     **plot_kwargs,\n",
    "                    )\n",
    "        plt.axhline(df[f's{si}_acc'][0], ls = '--', c = colors[axi], label=f'default S{si} acceptance')\n",
    "        plt.ylabel(f'S{si} acceptance')\n",
    "        plt.gca().yaxis.label.set_color(colors[axi])\n",
    "        plt.gca().tick_params(axis='y', colors=colors[axi])\n",
    "\n",
    "        if axi==0:\n",
    "            plt.xticks(df[mask]['number'], \n",
    "                       df[mask]['config'], \n",
    "                       rotation = 45, ha='right')\n",
    "\n",
    "    fig.legend(loc=5)\n",
    "    # Save the figure\n",
    "    pema.save_canvas(f'{config_type}_update_lowe_scan', \n",
    "                     save_dir=os.path.join(fig_dir, 'update_scan'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for config_type in np.unique(df['config_type'].values):\n",
    "    if not config_type:\n",
    "        continue\n",
    "    print(config_type)\n",
    "    # Select the configurations from the results dataframe\n",
    "    mask = np.array([c == config_type for c in  df['config_type']])\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(1*np.sum(mask),6))\n",
    "    plt.title(f'S1/S2 acceptance - {config_type}')\n",
    "    for axi, si in enumerate([1, 2]):\n",
    "        if axi ==1:\n",
    "            plt.sca(plt.gca().twinx())\n",
    "            plt.xticks()\n",
    "        plt.errorbar(df[mask]['number'], \n",
    "                     df[mask][f's{si}_bias'], \n",
    "                     yerr=df[mask][f's{si}_bias_err'],\n",
    "                     label = f'S{si} bias',\n",
    "                     c = colors[axi],\n",
    "                     **plot_kwargs,\n",
    "                    )\n",
    "        plt.axhline(df[f's{si}_bias'][0], ls = '--', c = colors[axi], label=f'default S{si} acceptance')\n",
    "        plt.ylabel(f'S{si} bias')\n",
    "        plt.gca().yaxis.label.set_color(colors[axi])\n",
    "        plt.gca().tick_params(axis='y', colors=colors[axi])\n",
    "        \n",
    "        if axi==0:\n",
    "            plt.xticks(df[mask]['number'], \n",
    "                       df[mask]['config'], \n",
    "                       rotation = 45, ha='right')\n",
    "    fig.legend(loc=5)\n",
    "    # Save the figure\n",
    "    pema.save_canvas(f'{config_type}_update_lowe_bias_scan', save_dir=os.path.join(fig_dir, 'update_scan'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select custom config\n",
    "## <span style=\"color:red\">**User input required**</span> \n",
    "From the plots above, we can now extract a desired change to the default clustering and classification parameters.\n",
    "\n",
    "An improved parameter:\n",
    " 1. Reduces the bias (i.e. brings it closer to a value of `1.`)\n",
    " 2. Increases the acceptance\n",
    " 3. Works for S1/S2 and for lowE / HighE\n",
    "\n",
    "Here, obviously, item `3.` is by far the most complex criterion to fullfil. Most parameters are quite optimized and have conflicting effects on e.g.g the S1- vs. S2-acceptance. As such, this is a step that needs to be done by a user since this weighing of interests is a complex task.\n",
    "\n",
    "### Example\n",
    "Below we are going to set the extracted config to\n",
    "```python\n",
    "summary_config = {\n",
    "    's2_merge_max_duration': 40000,\n",
    "    's2_merge_max_gap': 9000,\n",
    "    'peaklet_gap_threshold': 1000\n",
    "                 }\n",
    "```\n",
    "This config is not actually better than the current default settings but serves the purpose of being an example. \n",
    "\n",
    "Using this example, we make some detailed plots of how this config affects the bias, waveforms and acceptance. We keep the results of default (`st`) and custom (`st2`) outcomes side by side in two contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_config = {\n",
    "    's2_merge_max_duration': 40000,\n",
    "    's2_merge_max_gap': 9000,\n",
    "    'peaklet_gap_threshold': 1000\n",
    "                 }\n",
    "st2 = st.new_context()\n",
    "st2.set_config(summary_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "default_acceptence = st.get_array(selected_runs, 'match_acceptance_extended',progress_bar=False)\n",
    "custom_acceptence = st2.get_array(selected_runs, 'match_acceptance_extended',progress_bar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acceptance comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def si_acceptance(si, binedges, on_axis='n_photon', nbins=50):\n",
    "    mask = default_acceptence['type'] == si\n",
    "    pema.summary_plots.acceptance_plot(\n",
    "        default_acceptence[mask], \n",
    "        on_axis, \n",
    "        binedges, \n",
    "        nbins=nbins, \n",
    "        plot_label=default_label,\n",
    "    )\n",
    "    mask = custom_acceptence['type'] == si\n",
    "    pema.summary_plots.acceptance_plot(\n",
    "        custom_acceptence[mask], \n",
    "        on_axis, \n",
    "        binedges, \n",
    "        nbins=nbins, \n",
    "        plot_label=custom_label,\n",
    "    )\n",
    "    plt.ylabel('Arb. Acceptance')\n",
    "    plt.title(f\"S{si} acceptance\")\n",
    "    plt.legend()\n",
    "\n",
    "def acceptance_summary(si, on_axis, axis_label, nbins = 100, plot_range = (0, 200), save_name=''):\n",
    "    f, axes = plt.subplots(3, 1, figsize=(10,12), sharex=True)\n",
    "    max_photons = 35\n",
    "    plt.sca(axes[0])\n",
    "    sel = ((default_acceptence['type'] == si) \n",
    "           & (default_acceptence[on_axis] > plot_range[0])\n",
    "           & (default_acceptence[on_axis] < plot_range[1])\n",
    "          )\n",
    "    pema.summary_plots.plot_peak_matching_histogram(default_acceptence[sel], on_axis, bin_edges = nbins)\n",
    "    plt.text(0.05,0.95, \n",
    "             default_label,\n",
    "             transform=plt.gca().transAxes,\n",
    "             ha = 'left',\n",
    "             va = 'top',\n",
    "             bbox=dict(boxstyle=\"round\", fc=\"w\")\n",
    "            )\n",
    "    plt.legend(loc=(1.01,0))\n",
    "    plt.xlim(*plot_range)\n",
    "  \n",
    "    plt.sca(axes[1])\n",
    "    sel = ((custom_acceptence['type'] == si) \n",
    "           & (custom_acceptence[on_axis] > plot_range[0])\n",
    "           & (custom_acceptence[on_axis] < plot_range[1])\n",
    "          )\n",
    "    print(f'cust {np.sum(sel)}')\n",
    "    pema.summary_plots.plot_peak_matching_histogram(custom_acceptence[sel], on_axis, bin_edges = nbins)\n",
    "    plt.text(0.05,0.95, \n",
    "             custom_label,\n",
    "             transform=plt.gca().transAxes,\n",
    "             ha = 'left',\n",
    "             va = 'top',\n",
    "             bbox=dict(boxstyle=\"round\", fc=\"w\")\n",
    "            )\n",
    "    plt.legend(loc=(1.01,0))\n",
    "    plt.xlim(*plot_range)\n",
    "    \n",
    "    plt.sca(axes[2])\n",
    "    mask = default_acceptence['type'] == si\n",
    "    pema.summary_plots.acceptance_plot(default_acceptence[mask], on_axis, plot_range, nbins=nbins, \n",
    "                                       plot_label=default_label)\n",
    "    mask = custom_acceptence['type'] == si\n",
    "\n",
    "    pema.summary_plots.acceptance_plot(custom_acceptence[mask], on_axis, plot_range, nbins=nbins, \n",
    "                                       plot_label=custom_label)\n",
    "    plt.legend(loc=(1.01,0))\n",
    "    plt.ylabel('Arb. acceptance faction')\n",
    "    plt.xlim(*plot_range)\n",
    "    plt.xlabel(axis_label)\n",
    "    plt.ylim(0,1)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    plt.suptitle(f'S{si} Acceptance', y=0.9)\n",
    "    pema.save_canvas(f'{si}_acceptance_detailed_{save_name}', save_dir=fig_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptance_summary(si = 1, \n",
    "                   on_axis = 'n_photon',\n",
    "                   axis_label = 'N photons simulated', \n",
    "                   nbins = 100, \n",
    "                   plot_range = (0, 50),\n",
    "                   save_name = 'tot_compare',)\n",
    "\n",
    "acceptance_summary(si = 2, \n",
    "                   on_axis = 'n_photon',\n",
    "                   axis_label = 'N photons simulated', \n",
    "                   nbins = 100, \n",
    "                   plot_range = (0, 250),\n",
    "                  save_name = 'tot_compare')\n",
    "\n",
    "acceptance_summary(si = 2, \n",
    "                   on_axis = 'z',\n",
    "                   axis_label = 'z (simulated) [cm]', \n",
    "                   nbins = 75, \n",
    "                   plot_range = (-160, 10),\n",
    "                   save_name = 'tot_compare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reconstruction bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_max = default_acceptence['n_photon'][default_acceptence['type']==1].max()\n",
    "s2_max = default_acceptence['n_photon'][default_acceptence['type']==2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pema.summary_plots.rec_diff(\n",
    "    default_acceptence,\n",
    "    custom_acceptence,\n",
    "    s1_kwargs=dict(bins=[100, 100], range=[[0,s1_max], [0.6, 1.1]]),\n",
    "    s2_kwargs=dict(bins=[100, 100], range=[[0,s2_max], [0.6, 1.1]])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare wavefroms! This is very important to see where the improvements are coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pema.compare_outcomes(st, default_acceptence,                 \n",
    "                      st2, custom_acceptence,\n",
    "                      only_different=False,\n",
    "                      plot_fuzz=3000,\n",
    "                      fuzz=0,\n",
    "                      max_peaks=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {
    "a26e7f9ab967429687376a4127408b3b": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
